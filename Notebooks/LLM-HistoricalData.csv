Model’s Name,Year,Architecture Family,Parameters,Pre-Trained Data Scale,Modality,Reported Compute,Weight Size (Bytes),Training Duration,Hardware Specifications,Context Learning,Citation,
BERT(Large),2018,Encoder-Only,340 M,3.3B words,Text,~1 PF-day,1.3 GB,4 days,"Nvidia A100, V100",Yes,"Devlin et al., 2018",
RoBERTa(Large),2019,Encoder-Only,355 M,160 GB Text,Text,–,1.3 GB,≈ 2 weeks,6144 TPU v4,Yes,"Liu et al., 2019",
GPT-3,2020,Decoder-Only,175 B,300 B tokens,Text,3.64 × 10²³ FLOPs,~350 GB,≈ 1 month,Nvidia A100 GPU,Yes,"Brown et al., 2020",
T5-11b,2020,Encoder-Decoder,11 B,1 T tokens,Text,–,44 GB,–,1024 TPU v3,Yes,"Raffel et al., 2020",
PaLM,2022,Decoder-Only,540 B,780 B tokens,Text + Code,2.56 x 10^24 FLOPs,~1.1 TB,120 days,6144 TPU v4,Yes,"Chowdhery et al., 2022",
LaMDA,2022,Decoder-Only,137 B,768 B tokens,Dialogue,–,~548 GB,57.7 days,1024 TPU v3,Yes,"Thoppilan et al., 2022",
GLM-130B,2022,Decoder-Only,130 B,400 B tokens,Text,–,520 GB,60 days,Nvidia A100,Yes,"Zeng et al., 2022",
Gopher,2021,Decoder-Only,280 B,300 B tokens,Text,–,1.12 TB,920 hrs,4096 TPU v3,Yes,"Rae et al., 2021",
Jurassic-1,2021,Decoder-Only,178 B,300 B tokens,Text,–,712 GB,–,800 GPU,Yes,"Lieber et al., 2021",
MT-NLG,2022,Decoder-Only,530 B,270 B tokens,Text,–,~2.1 TB,–,4480 A100 80 GB,Yes,"Smith et al., 2022",
LLaMA 2,2023,Decoder-Only,70 B,2 T tokens,Text,–,140 GB,21 days,2000 A100 80 GB,Yes,"Touvron et al., 2023",
Falcon,2023,Decoder-Only,40 B,1.3 T tokens,Text,–,80 GB,–,Nvidia A100,Yes,"Penedo et al., 2023",
Chinchilla,2022,Decoder-Only,70 B,1.4 T tokens,Text,–,140 GB,–,TPU v4,Yes,"Hoffmann et al., 2022",
Galactica,2022,Decoder-Only,120 B,106 B tokens,Scientific Text,–,480 GB,–,992 A100 80 GB,Yes,"Taylor et al., 2022",
BLOOM,2022,Decoder-Only,176 B,366 B tokens,Text (Multilingual),–,704 GB,105 days,384 A100 80 GB,Yes,"BigScience, 2022",
PanGu-α,2021,Decoder-Only,207 B,1.1 TB text,Text,–,~828 GB,–,2048 Ascend 910,Yes,"Zeng et al., 2021",
GPT-4,2023,Mixture-of-Experts (Decoder-Only),≈ 1 T (est.),Undisclosed,Text + Image,Confidential,Undisclosed,Confidential,Azure AI Supercluster,Yes,"OpenAI, 2023",
Claude 3 Opus,2024,Transformer (Decoder-Only),– (>100 B est.),Proprietary Corpus,Text + Image,Confidential,–,–,Anthropic Cluster v2,Yes,"Anthropic, 2024",
Gemini 1.5,2024,Multimodal Transformer,1.5 T,Web + Vision + Code Corpus,Text + Image + Code,~1e24 FLOPs,~3 TB,Confidential,TPU v5e Pods,Yes,"DeepMind, 2024",
LLaMA 3 70B,2024,Decoder-Only,70 B,15 T tokens,Text + Code,–,140 GB,–,A100 80 GB,Yes,"Meta AI, 2024",
DeepSeek-V3,2024,Mixture-of-Experts,671 B (37 Active),14.8 T tokens,Text + Code + Reasoning,2.788 M GPU hrs,~1.34 TB,–,H800 GPU Cluster,Yes,"DeepSeek Tech, 2024",
DeepSeek-R1,2025,Mixture-of-Experts (Reasoning),671 B (37 Active),14.8 T tokens,Text + Reasoning,–,~1.34 TB,–,Confidential (H800 Cluster),Yes,"DeepSeek Tech, 2025",
GPT-5,2025,Unified system (fast + reasoning + router),Undisclosed,Undisclosed,Text + Image (multimodal),Undisclosed,Undisclosed,–,Undisclosed,Yes,"OpenAI, 2025",
Gemini 2.5 Pro,2025,Multimodal Transformer / “Thinking” model,Undisclosed,Undisclosed,Text + Image + Audio/Video,Undisclosed,Undisclosed,–,Undisclosed,Yes,"DeepMind, 2025",
Claude 4 (Opus),2025,Advanced reasoning model,Undisclosed,Undisclosed,Text + Image (vision supported),Undisclosed,Undisclosed,–,Undisclosed,Yes,"Anthropic, 2025",
Grok 4,2025,Agentic multimodal reasoning model,Undisclosed,Undisclosed,Text + Reasoning + Tool Use,Undisclosed,Undisclosed,–,Undisclosed,Yes,"Grok 4 (xAI),2025",
,,,,,,,,,,,,
,,,,,,,,,,,,
