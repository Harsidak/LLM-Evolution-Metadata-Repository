This project followed a structured process to ensure transparency, consistency, and reproducibility. All model information was collected from primary sources such as research papers, technical reports, and official model documentation.

1. Data Collection
Model attributes—release year, architecture type, parameter count, training data scale, compute estimates, and modality—were extracted from peer-reviewed papers, arXiv preprints, and verified developer sources. Each data point was cross-checked to avoid inaccuracies.

2. Metadata Corpus Construction
All collected information was cleaned, standardized, and organized into a single metadata table. The dataset was exported into both CSV and JSON formats to support human readability and programmatic use.

3. Taxonomy Development
Models were grouped by architecture family (encoder-only, decoder-only, encoder–decoder), parameter scale, and modality. This taxonomy was designed to highlight structural differences and their relationship to capability progression.

4. Timeline Generation
A chronological sequence of models was constructed based on confirmed release years. This timeline was later used to analyze trend progression across different generations of LLMs.

5. Trend Analysis
The metadata was evaluated to identify patterns in parameter scaling, compute demands, emergent capabilities, and architectural evolution. Gaps and unresolved challenges were recorded to guide future research stages.