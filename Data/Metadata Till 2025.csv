Model’s Name,Year,Architecture Family,Parameters,Pre-Trained Data Scale,Modality,Reported Compute,Weight Size (Bytes),Training Duration,Hardware Specifications,Context Learning,Citation,
GPT-4,2023,Mixture-of-Experts (Decoder-Only),≈ 1 T (est.),Undisclosed,Text + Image,Confidential,Undisclosed,Confidential,Azure AI Supercluster,Yes,"OpenAI, 2023",
Claude 3 Opus,2024,Transformer (Decoder-Only),– (>100 B est.),Proprietary Corpus,Text + Image,Confidential,–,–,Anthropic Cluster v2,Yes,"Anthropic, 2024",
Gemini 1.5,2024,Multimodal Transformer,1.5 T,Web + Vision + Code Corpus,Text + Image + Code,~1e24 FLOPs,~3 TB,Confidential,TPU v5e Pods,Yes,"DeepMind, 2024",
LLaMA 3 70B,2024,Decoder-Only,70 B,15 T tokens,Text + Code,–,140 GB,–,A100 80 GB,Yes,"Meta AI, 2024",
DeepSeek-V3,2024,Mixture-of-Experts,671 B (37 Active),14.8 T tokens,Text + Code + Reasoning,2.788 M GPU hrs,~1.34 TB,–,H800 GPU Cluster,Yes,"DeepSeek Tech, 2024",
DeepSeek-R1,2025,Mixture-of-Experts (Reasoning),671 B (37 Active),14.8 T tokens,Text + Reasoning,–,~1.34 TB,–,Confidential (H800 Cluster),Yes,"DeepSeek Tech, 2025",
GPT-5,2025,Unified system (fast + reasoning + router),Undisclosed,Undisclosed,Text + Image (multimodal),Undisclosed,Undisclosed,–,Undisclosed,Yes,"OpenAI, 2025",
Gemini 2.5 Pro,2025,Multimodal Transformer / “Thinking” model,Undisclosed,Undisclosed,Text + Image + Audio/Video,Undisclosed,Undisclosed,–,Undisclosed,Yes,"DeepMind, 2025",
Claude 4 (Opus),2025,Advanced reasoning model,Undisclosed,Undisclosed,Text + Image (vision supported),Undisclosed,Undisclosed,–,Undisclosed,Yes,"Anthropic, 2025",
Grok 4,2025,Agentic multimodal reasoning model,Undisclosed,Undisclosed,Text + Reasoning + Tool Use,Undisclosed,Undisclosed,–,Undisclosed,Yes,"Grok 4 (xAI),2025",
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
