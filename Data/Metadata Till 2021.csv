Model’s Name,Year,Architecture Family,Parameters,Pre-Trained Data Scale,Modality,Reported Compute,Weight Size (Bytes),Training Duration,Hardware Specifications,Context Learning,Citation,
BERT(Large),2018,Encoder-Only,340 M,3.3B words,Text,~1 PF-day,1.3 GB,4 days,"Nvidia A100, V100",Yes,"Devlin et al., 2018",
RoBERTa(Large),2019,Encoder-Only,355 M,160 GB Text,Text,–,1.3 GB,≈ 2 weeks,6144 TPU v4,Yes,"Liu et al., 2019",
GPT-3,2020,Decoder-Only,175 B,300 B tokens,Text,3.64 × 10²³ FLOPs,~350 GB,≈ 1 month,Nvidia A100 GPU,Yes,"Brown et al., 2020",
T5-11b,2020,Encoder-Decoder,11 B,1 T tokens,Text,–,44 GB,–,1024 TPU v3,Yes,"Raffel et al., 2020",
PaLM,2022,Decoder-Only,540 B,780 B tokens,Text + Code,2.56 x 10^24 FLOPs,~1.1 TB,120 days,6144 TPU v4,Yes,"Chowdhery et al., 2022",
LaMDA,2022,Decoder-Only,137 B,768 B tokens,Dialogue,–,~548 GB,57.7 days,1024 TPU v3,Yes,"Thoppilan et al., 2022",
GLM-130B,2022,Decoder-Only,130 B,400 B tokens,Text,–,520 GB,60 days,Nvidia A100,Yes,"Zeng et al., 2022",
Gopher,2021,Decoder-Only,280 B,300 B tokens,Text,–,1.12 TB,920 hrs,4096 TPU v3,Yes,"Rae et al., 2021",
Jurassic-1,2021,Decoder-Only,178 B,300 B tokens,Text,–,712 GB,–,800 GPU,Yes,"Lieber et al., 2021",
MT-NLG,2022,Decoder-Only,530 B,270 B tokens,Text,–,~2.1 TB,–,4480 A100 80 GB,Yes,"Smith et al., 2022",
LLaMA 2,2023,Decoder-Only,70 B,2 T tokens,Text,–,140 GB,21 days,2000 A100 80 GB,Yes,"Touvron et al., 2023",
Falcon,2023,Decoder-Only,40 B,1.3 T tokens,Text,–,80 GB,–,Nvidia A100,Yes,"Penedo et al., 2023",
Chinchilla,2022,Decoder-Only,70 B,1.4 T tokens,Text,–,140 GB,–,TPU v4,Yes,"Hoffmann et al., 2022",
Galactica,2022,Decoder-Only,120 B,106 B tokens,Scientific Text,–,480 GB,–,992 A100 80 GB,Yes,"Taylor et al., 2022",
BLOOM,2022,Decoder-Only,176 B,366 B tokens,Text (Multilingual),–,704 GB,105 days,384 A100 80 GB,Yes,"BigScience, 2022",
PanGu-α,2021,Decoder-Only,207 B,1.1 TB text,Text,–,~828 GB,–,2048 Ascend 910,Yes,"Zeng et al., 2021",
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
,,,,,,,,,,,,
